# -*- coding: utf-8 -*-
"""KLASIFIKASI FIKS DONG

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iRR3bIroKwinAZEKCwKH4gQVtz6DlFxr
"""

import zipfile
import os

# Path file ZIP
zip_path = "Fire_Disaster.zip"

with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall(".")

print("File ZIP berhasil diekstrak")
print("Isi folder sekarang:", os.listdir("."))

# import libray & split data
import os
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, classification_report
import seaborn as sns
import random
import shutil
from PIL import Image


SOURCE_DIR = "Fire_Disaster"
TARGET_DIR = "fire_split"

TRAIN_RATIO = 0.7
VAL_RATIO = 0.15
TEST_RATIO = 0.15

classes = ["Wild_Fire", "Urban_Fire"]

def is_image_file(filepath):
    try:
        img = Image.open(filepath)
        img.verify()
        return True
    except:
        return False

for cls in classes:
    os.makedirs(f"{TARGET_DIR}/train/{cls}", exist_ok=True)
    os.makedirs(f"{TARGET_DIR}/val/{cls}", exist_ok=True)
    os.makedirs(f"{TARGET_DIR}/test/{cls}", exist_ok=True)

    images = []
    for img_name in os.listdir(os.path.join(SOURCE_DIR, cls)):
        path = os.path.join(SOURCE_DIR, cls, img_name)
        if os.path.isfile(path) and is_image_file(path):
            images.append(img_name)

    random.shuffle(images)
    total = len(images)
    train_end = int(TRAIN_RATIO * total)
    val_end = int((TRAIN_RATIO + VAL_RATIO) * total)

    for i, img in enumerate(images):
        src = os.path.join(SOURCE_DIR, cls, img)
        if i < train_end:
            dst = f"{TARGET_DIR}/train/{cls}/{img}"
        elif i < val_end:
            dst = f"{TARGET_DIR}/val/{cls}/{img}"
        else:
            dst = f"{TARGET_DIR}/test/{cls}/{img}"
        shutil.copy(src, dst)

print("Dataset split selesai")

# konfigurasi
IMG_SIZE = (224, 224)
BATCH_SIZE = 32
EPOCHS = 10
SEED = 42
DATA_DIR = "fire_split"

# augmentasi data
data_augmentation = tf.keras.Sequential([
    tf.keras.layers.RandomFlip("horizontal"),
    tf.keras.layers.RandomRotation(0.1),
    tf.keras.layers.RandomZoom(0.1),
])

def load_dataset(path, shuffle=True, augment=False):
    ds = tf.keras.utils.image_dataset_from_directory(
        path,
        image_size=IMG_SIZE,
        batch_size=BATCH_SIZE,
        shuffle=shuffle,
        seed=SEED
    )
    if augment:
        ds = ds.map(lambda x, y: (data_augmentation(x), y))
    ds = ds.ignore_errors()
    return ds.prefetch(tf.data.AUTOTUNE)

train_ds = load_dataset(os.path.join(DATA_DIR, "train"), augment=True)
val_ds   = load_dataset(os.path.join(DATA_DIR, "val"))
test_ds  = load_dataset(os.path.join(DATA_DIR, "test"), shuffle=False)

print("Dataset siap")

# cnn
cnn_model = tf.keras.Sequential([
    tf.keras.layers.Rescaling(1./255, input_shape=(224,224,3)),
    tf.keras.layers.Conv2D(32, 3, activation='relu'),
    tf.keras.layers.MaxPooling2D(),
    tf.keras.layers.Conv2D(64, 3, activation='relu'),
    tf.keras.layers.MaxPooling2D(),
    tf.keras.layers.Conv2D(128, 3, activation='relu'),
    tf.keras.layers.MaxPooling2D(),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dense(1, activation='sigmoid')
])

cnn_model.compile(
    optimizer='adam',
    loss='binary_crossentropy',
    metrics=['accuracy']
)

print("\nCNN MODEL SUMMARY")
cnn_model.summary()

history_cnn = cnn_model.fit(
    train_ds,
    validation_data=val_ds,
    epochs=EPOCHS
)

# transfer learning
base_model = tf.keras.applications.MobileNetV2(
    input_shape=(224,224,3),
    include_top=False,
    weights='imagenet'
)
base_model.trainable = False

tl_model = tf.keras.Sequential([
    tf.keras.layers.Rescaling(1./255),
    base_model,
    tf.keras.layers.GlobalAveragePooling2D(),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dense(1, activation='sigmoid')
])

tl_model.compile(
    optimizer='adam',
    loss='binary_crossentropy',
    metrics=['accuracy']
)

print("\nTRANSFER LEARNING MODEL SUMMARY")
tl_model.summary()

history_tl = tl_model.fit(
    train_ds,
    validation_data=val_ds,
    epochs=EPOCHS
)

# grfk prbndingan acc, eval, confusion matrix
plt.figure(figsize=(10,5))
plt.plot(history_cnn.history['val_accuracy'], label="CNN Validation Accuracy")
plt.plot(history_tl.history['val_accuracy'], label="TL Validation Accuracy")
plt.title("Perbandingan Validation Accuracy")
plt.xlabel("Epoch")
plt.ylabel("Accuracy")
plt.legend()
plt.show()

def evaluate_model(model, name):
    y_true, y_pred = [], []

    for images, labels in test_ds:
        preds = model.predict(images)
        preds = (preds > 0.5).astype(int)
        y_true.extend(labels.numpy())
        y_pred.extend(preds.flatten())

    print(f"\n Classification Report - {name}")
    print(classification_report(
        y_true, y_pred,
        target_names=["Urban Fire", "Wild Fire"]
    ))

    cm = confusion_matrix(y_true, y_pred)
    plt.figure(figsize=(5,4))
    sns.heatmap(
        cm, annot=True, fmt="d", cmap="Blues",
        xticklabels=["Urban", "Wild"],
        yticklabels=["Urban", "Wild"]
    )
    plt.title(f"Confusion Matrix - {name}")
    plt.xlabel("Predicted")
    plt.ylabel("Actual")
    plt.show()

evaluate_model(cnn_model, "CNN")

evaluate_model(tl_model, "Transfer Learning")